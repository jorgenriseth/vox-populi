{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vox Populi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start out with creating a json-document with information about MP's on twitter retreived from https://www.mpsontwitter.co.uk/list. There are some errors within the list, and these errors are fixed using the user_id_correction-function loading in corrections from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correction\n",
      "Old:  {'party': 'Conservative', 'screen_name': 'daviddaguidmp', 'url': 'https://twitter.com/daviddaguidmp'}\n",
      "New:  {'party': 'Conservative', 'screen_name': 'davidduguidmp', 'url': 'https://twitter.com/davidduguidmp'}\n",
      "\n",
      "\n",
      "Correction\n",
      "Old:  {'party': 'Conservative', 'screen_name': 'eorgeFreemanMP', 'url': 'https://twitter.com/eorgeFreemanMP'}\n",
      "New:  {'party': 'Conservative', 'screen_name': 'GeorgeFreemanMP', 'url': 'https://twitter.com/GeorgeFreemanMP'}\n",
      "\n",
      "\n",
      "Correction\n",
      "Old:  {'party': 'Conservative', 'screen_name': 'SirRogerGaleMP', 'url': 'https://twitter.com/SirRogerGaleMP'}\n",
      "New:  {'party': 'Conservative', 'screen_name': 'SirRogerGale', 'url': 'https://twitter.com/SirRogerGale'}\n",
      "\n",
      "MPs on Twitter stored in mps.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_user_list(filename='mps.json', errata='user-errata.txt'):\n",
    "    \"\"\"\n",
    "    Retrieves list of MP's on twitter, and store their username,\n",
    "    political party, and twitter profile url in a json file,\n",
    "    their full names as keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.mpsontwitter.co.uk/list\"\n",
    "    data = requests.get(url)\n",
    "\n",
    "    html = BeautifulSoup(data.text, 'html.parser')\n",
    "    table = html.select(\"tbody\", id='mp_wrapper')[1]\n",
    "\n",
    "    mp_dict = {}\n",
    "    for line in table.select('tr'):\n",
    "        name = line.select('td')[2].get_text().strip()\n",
    "        party = ' '.join(line.td['class'])\n",
    "        twitter_id = line.a.get_text()[1:]\n",
    "        url = line.a['href']\n",
    "            \n",
    "        mp_dict[name] = {\n",
    "            \"party\": party,\n",
    "            \"screen_name\": twitter_id,\n",
    "            \"url\": \"https://twitter.com/\" + twitter_id\n",
    "        }\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(mp_dict, f)\n",
    "        \n",
    "    if errata is not None:\n",
    "        user_id_correction(erratafile=errata, user_file=filename)\n",
    "        \n",
    "    print(f'MPs on Twitter stored in {filename}')\n",
    "    \n",
    "    \n",
    "def user_id_correction(erratafile='user-errata.txt',\n",
    "                       user_file='mps.json'):\n",
    "    \"\"\"\n",
    "    Fixes errors in user_file, using information stored in erratafile\n",
    "    \"\"\"\n",
    "    # Create dictionary with errata[wrong-id] = errata[righ-id]\n",
    "    with open(erratafile, 'r') as f:\n",
    "        errata = {}\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            errata[line.split('=')[0]] = line.split('=')[1]\n",
    "\n",
    "    # Load dictironary to be corrected\n",
    "    with open(user_file, 'r') as f:\n",
    "        user_dict = json.load(f)\n",
    "    \n",
    "    \n",
    "    # Correct the entries in the user-file\n",
    "    for wrong_id, right_id in errata.items():\n",
    "        for name, info in user_dict.items():\n",
    "            if info[\"screen_name\"] == wrong_id:\n",
    "                print()\n",
    "                print('Correction')\n",
    "                print('Old: ', user_dict[name])\n",
    "                user_dict[name][\"screen_name\"] = right_id\n",
    "                user_dict[name][\"url\"] = info[\"url\"][:-len(wrong_id)] + right_id\n",
    "                print('New: ', user_dict[name])\n",
    "                print()\n",
    "\n",
    "    # Save corrected dictionary to json.\n",
    "    with open(user_file, 'w') as f:\n",
    "        json.dump(user_dict, f)\n",
    "    \n",
    "create_user_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this list of MP's we will load their tweets into a corpus using the tweepy-module. It will need to use the API-key that you should have stoed in the credentials.txt-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tweepy\n",
    "\n",
    "class CorpusCreator:\n",
    "    \"\"\"\n",
    "    Class for creating corpus structure and loading tweets\n",
    "    using tweepy.\n",
    "    \n",
    "    Structure:\n",
    "    -> corpus\n",
    "    \n",
    "    ---> party1\n",
    "    -----> user11\n",
    "    -----> user12\n",
    "    -----> user13\n",
    "    \n",
    "    ---> party2\n",
    "    -----> user21\n",
    "    -----> user22\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_dict=None, rel_path='./',\n",
    "                rate_limit_wait=True):\n",
    "        \n",
    "        # Get an API-object authorized from 'credentials.txt'\n",
    "        auth = get_tweet_auth()\n",
    "        self.api = tweepy.API(auth,\n",
    "                              wait_on_rate_limit=rate_limit_wait,\n",
    "                              wait_on_rate_limit_notify=rate_limit_wait)\n",
    "        \n",
    "        self.root = rel_path + 'corpus/'\n",
    "        \n",
    "        # Load mp-file from directory\n",
    "        assert type(user_dict) in (str, dict) or user_dict is None,\"User_dict wrong format\"\n",
    "        if user_dict is None:\n",
    "            user_dict = 'mps.json'\n",
    "            \n",
    "        if type(user_dict) is str:\n",
    "            with open(user_dict) as f:\n",
    "                self.users = json.load(f)\n",
    "                \n",
    "        elif type(user_dict) is dict:\n",
    "            self.users = user_dict\n",
    "        \n",
    "        # Create root filesystem\n",
    "        try:\n",
    "            os.mkdir(self.root)\n",
    "            print('Directory \"corpus created.')\n",
    "            print()\n",
    "        except:\n",
    "            print('Directory \"corpus\" already exists.')\n",
    "            print()\n",
    "            \n",
    "        \n",
    "            \n",
    "    def load_tweets(self, max_items=10000, user=None):\n",
    "        \"\"\"\n",
    "        For all users in self.users, get [max_items] tweets and\n",
    "        save each to separate files. \n",
    "        \"\"\"\n",
    "        for name, info in self.users.items():\n",
    "            try:\n",
    "                os.mkdir(self.root + info['party'].lower().replace(' ', '_'))\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            \n",
    "            filepath = self.root + info['party'].lower().replace(' ', '_')\n",
    "            filepath = filepath + '/' + name.lower().replace(' ', '')\n",
    "            try:\n",
    "                print(f'Reading tweets from {name}')\n",
    "                user = info['screen_name']\n",
    "                curs = tweepy.Cursor(self.api.user_timeline,\n",
    "                                     screen_name=user,\n",
    "                                     count=200,\n",
    "                                     tweet_mode=\"extended\"\n",
    "                                     ).items(max_items)\n",
    "\n",
    "                with open(filepath + '.jsonl', 'w') as f:\n",
    "                    for status in curs:\n",
    "                        tweet = status._json\n",
    "                        json_dump_line(tweet, f)\n",
    "                        \n",
    "            except tweepy.TweepError as exc:\n",
    "                print(exc)\n",
    "                os.remove(filepath + '.jsonl')\n",
    "\n",
    "                \n",
    "def get_tweet_auth(auth_file='credentials.txt'):\n",
    "    \"\"\"\n",
    "    Get tweepy oauth object given a credentials-file, formatted\n",
    "    as a nltk.twitter-creds file.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    \n",
    "    # Open credentials-file\n",
    "    with open(auth_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Read only key/token\n",
    "            token = line.split('=')[-1].rstrip('\\n')\n",
    "            \n",
    "            # Add token to keys-list\n",
    "            if token is not '':\n",
    "                keys.append(line.split('=')[-1].rstrip('\\n'))\n",
    "                \n",
    "    auth = tweepy.OAuthHandler(*keys[:2])\n",
    "    auth.set_access_token(*keys[2:])\n",
    "    return auth\n",
    "\n",
    "\n",
    "def json_dump_line(json_object, file_object):\n",
    "    \"\"\"\n",
    "    Dumps a dictionay json_object to file_object, adding \n",
    "    a trailing newline, hence creating a json line format.\n",
    "    \"\"\"\n",
    "    json.dump(json_object, file_object)\n",
    "    file_object.write('\\n')\n",
    "\n",
    "\n",
    "def rm_empty_json_in_path(path):\n",
    "    \"\"\"\n",
    "    Browses through corpus-files and removes any user.json-files which are \n",
    "    empty for various reasons.\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(path), \"[path] not a valid directory\"\n",
    "    \n",
    "    # Ensure directories are given with ending '/' for recursion\n",
    "    if path[-1] != '/':\n",
    "        path += '/'\n",
    "    \n",
    "    print('Browsing \"' + path + '\"')\n",
    "        \n",
    "    for f in os.listdir(path):\n",
    "        filepath = path + f\n",
    "        if os.path.isfile(filepath) and '.jsonl' in filepath:\n",
    "            try:\n",
    "                if os.path.getsize(filepath) == 0:\n",
    "                    print('Removing ' + filepath)\n",
    "                    os.remove(filepath)\n",
    "            \n",
    "            # Shouldn't happen, but just to make sure.\n",
    "            except OSError as e:\n",
    "                print(e)\n",
    "                pass\n",
    "        \n",
    "        elif os.path.isdir(filepath):\n",
    "            # Browse one dir deeper\n",
    "            rm_empty_json_in_path(path + f + '/')    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "number_of_tweets = int(input(\"Number of tweets per user:\"))\n",
    "\n",
    "# Ensure that the argument is a positive integer.\n",
    "assert number_of_tweets > 0, \"Number of tweets must be a positive integer\"\n",
    "\n",
    "# Create MP-list and run corpuscreator\n",
    "if not os.path.isfile('mps.json'):\n",
    "    create_user_list()\n",
    "\n",
    "corpus_creator = CorpusCreator(user_dict='mps.json')\n",
    "corpus_creator.load_tweets(max_items=number_of_tweets)\n",
    "rm_empty_json_in_path('corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some files generated that are empty for various reasons, e.g. mps that have created a twitter account, but have yet to post anything. These should be cleaned out to prevent errors when loading corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tweets are finished downloading, we will need to create a way to read them. We will build a class inheriting from the nltk TwitterCorpusReader. However, due to twitter extending maximum tweet length, we will have to make our own \"strings()\"-function ensuring that it reads the full tweets, instead of ending tweets with a '...' if they go beyond the canonical character-bound. Furthermore we will write the tweets to different dataframes, for easier use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus.reader import TwitterCorpusReader\n",
    "\n",
    "\n",
    "\n",
    "class MPTweetCorpusReader(TwitterCorpusReader):\n",
    "    \"\"\"\n",
    "    Class cerate specifically for ease of use in text clustering of the \n",
    "    British Member of Parliament tweets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, fileids=None, word_tokenizer=TweetTokenizer(),\n",
    "                 encoding='utf-8', create_df=False):\n",
    "        TwitterCorpusReader.__init__(self, root, fileids, word_tokenizer,\n",
    "                                     encoding)\n",
    "\n",
    "        self.parties = list(set([fileid.split('/')[0] for fileid in self.fileids()]))\n",
    "        self.users = [fileid.split('/')[1].split('.')[0] for fileid in self.fileids()]\n",
    "        \n",
    "        self.num_tweets = len(self.strings())\n",
    "        self.num_parties = len(self.parties)\n",
    "        self.num_users = len(self.users)\n",
    "        \n",
    "        \n",
    "        with open(self.root + '../mps.json') as f:\n",
    "            self._mp_dict = json.load(f)\n",
    "        \n",
    "        \n",
    "        self.df_savepath = self.root + 'tweet_df.pkl'\n",
    "        \n",
    "        if create_df:\n",
    "            print('Building tweet dataframe.')\n",
    "            self._build_dataframe()\n",
    "            print('Tweet dataframe built.')\n",
    "            print()\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                print(\"Loading tweet dataframe.\")\n",
    "                self.df = pd.read_pickle(self.df_savepath)\n",
    "                print(\"Tweet dataframe loaded.\")\n",
    "                print()\n",
    "            \n",
    "            except OSError as exc:\n",
    "                self.df = None\n",
    "                \n",
    "                print('OSError: ' + exc)\n",
    "                print(\"No dataframe created/loaded.\")\n",
    "                print()\n",
    "    \n",
    "    def strings(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns only the text content of Tweets in the file(s)\n",
    "        :return: the given file(s) as a list of Tweets.\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "        fulltweets = self.docs(fileids)\n",
    "        tweets = []\n",
    "        for jsono in fulltweets:\n",
    "            try:\n",
    "                text = jsono[\"full_text\"]\n",
    "                if isinstance(text, bytes):\n",
    "                    text = text.decode(self.encoding)\n",
    "                tweets.append(text)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return tweets\n",
    "                \n",
    "    def _build_dataframe(self):\n",
    "        self.df = pd.DataFrame(columns=['user', 'party', 'userid', 'text'])\n",
    "        i = -1    \n",
    "        for user, info in self._mp_dict.items():\n",
    "            # Get filepath for users tweet\n",
    "            user_fileids = (info['party'].lower().replace(' ', '_') + '/' \n",
    "                            + user.lower().replace(' ', '') + '.jsonl')\n",
    "\n",
    "            try:\n",
    "                user_tweets = self.strings(user_fileids)\n",
    "\n",
    "                for string in user_tweets:\n",
    "                    i += 1\n",
    "                    self.df.loc[i] = [user, info['party'], info['screen_name'], string]\n",
    "\n",
    "            except OSError:\n",
    "                # File doesn't exists, probably due to locked twitter profile.\n",
    "                pass\n",
    "\n",
    "            \n",
    "    def to_dataframe(self, samples='tweet', savename=None):\n",
    "        \"\"\"\n",
    "        samples = string: {'tweet', 'user', 'party'}\n",
    "        \n",
    "        Create a dataframe where each row reperesnts one tweet, and stores \n",
    "        as a member variable. If samples is 'user' or 'party', it will return \n",
    "        a dataframe for which all tweets belonging to a single user/party is \n",
    "        concatenated into one. \n",
    "        \"\"\"\n",
    "        assert samples in (\"tweet\", \"user\", \"party\"), \"Invalid argument [samples]:\" + str(samples)\n",
    "        \n",
    "        # Create base dataframe.\n",
    "        if samples == \"tweet\":\n",
    "            return self.df\n",
    "            \n",
    "        # Create \"lower resolution\" dataframes if necessary\n",
    "        if samples in (\"user\", \"party\"):\n",
    "            \n",
    "            df_by_user = pd.DataFrame(columns=['user', 'party', 'text'])\n",
    "            \n",
    "            i = -1\n",
    "            for user, info in self._mp_dict.items():\n",
    "                # Concatenate all tweets from user into one string.\n",
    "                tweets = ' '.join(list(self.df.loc[self.df['user'] == user]['text']))\n",
    "                \n",
    "                if tweets != '':\n",
    "                    i += 1\n",
    "                    df_by_user.loc[i] = [user, info['party'], tweets]\n",
    "                    \n",
    "            # Let name of mp/user be index of dataframe\n",
    "            df_by_user.set_index('user', inplace=True)\n",
    "            \n",
    "            if samples == \"party\":\n",
    "                df_by_party = pd.DataFrame(columns=['party', 'text'])\n",
    "                \n",
    "                i = -1\n",
    "                for party in df_by_user['party'].unique():\n",
    "                    # Concatenate tweets from all users in party into single string\n",
    "                    tweets = ' '.join(list(df_by_user.loc[df_by_user['party'] == party]['text']))\n",
    "                    \n",
    "                    if tweets != '':\n",
    "                        i += 1\n",
    "                        df_by_party.loc[i] = [party, tweets]\n",
    "                        \n",
    "                df_by_party.set_index('party', inplace=True)\n",
    "                        \n",
    "                if savename is not None:\n",
    "                    df_by_party.to_pickle(self.root + savename)\n",
    "                return df_by_party\n",
    "            \n",
    "            if savename is not None:\n",
    "                df_by_user.to_pickle(self.root + savename)\n",
    "            return df_by_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tweet dataframe.\n",
      "Tweet dataframe built.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = MPTweetCorpusReader(root='./corpus/', fileids='.*.jsonl', create_df=True)\n",
    "corpus.df.to_pickle('corpus/tweet_df.pkl')\n",
    "df_user = corpus.to_dataframe('user', 'user_df.pkl')\n",
    "df_party = corpus.to_dataframe('party', 'party_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
